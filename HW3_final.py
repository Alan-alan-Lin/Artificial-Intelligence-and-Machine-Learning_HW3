# # -*- coding: utf-8 -*-
# """Untitled1.ipynb

# Automatically generated by Colaboratory.

# Original file is located at
#     https://colab.research.google.com/drive/1dwZDxdHuf_5D_G3zMH7E_lYwQ9cdZ8pL
# """

 #-------import------------
import numpy as np
import pandas as pd
import time

def clear_output():
    """
    clear output for both jupyter notebook and the console
    """
    import os
    os.system('cls' if os.name == 'nt' else 'clear')
    if is_in_notebook():
        from IPython.display import clear_output as clear
        clear()

def is_in_notebook():
    import sys
    return 'ipykernel' in sys.modules

#-------global variable---------
true_interaction = " "
x_len=40  #y軸
y_len=30  #x軸
SPACE = np.zeros((x_len,y_len))
N_STATES = [ ss for ss in range(SPACE.size)]
N_STATES_dict = {}
i = 0
for x in range(x_len):
  for y in range(y_len):
    N_STATES_dict[i] = [x,y]
    i+=1
ACTIONS = ['left','right','up','down']     #可以做的動作
EPSILON = 0.9   # epsilon greedy
ALPHA = 0.1     # learning rate
GAMMA = 0.9    # discount factor
MAX_EPISODES = 50  # maximum episodes
FRESH_TIME = 0.01    # fresh time for one move
GOAL = [39,29]      #寶藏位置

#--------建立Q table----------
def build_q_table(n_states, actions): 
  table = pd.DataFrame(np.zeros((len(n_states), len(actions))),columns=actions,index = n_states)   
  return table

#--------choose action的功能-----------
def choose_action(state, q_table): 
    state_actions = q_table.iloc[state, :] #取state這一行的對應資料
    #act non-greedy or state-action have no value
    if (np.random.uniform() > EPSILON) or ((state_actions == 0).all()):  
        action_name = np.random.choice(ACTIONS) 
    else:   # act greedy
        action_name = state_actions.idxmax()
    return action_name

def get_key (dict, value):
 return [k for k, v in dict.items() if v == value]

#--------建立環境對我們行為的feedback---------
def get_env_feedback(S, A): 
  current_location = N_STATES_dict[S].copy()
  if (A == 'right') and (current_location[0]<(x_len-1)):
    current_location[0] += 1
  if (A == 'left') and (current_location[0]>0):
    current_location[0] -= 1
  if (A == 'up') and (current_location[1]>0):
    current_location[1] -= 1
  if (A == 'down') and (current_location[1]<(y_len-1)):
    current_location[1] += 1
 
  if current_location!=GOAL:
    R = 0 #reward
    S_ = get_key(N_STATES_dict,current_location)[-1]
  if current_location==GOAL: 
    R = 100
    S_ = 'terminal'
  return S_ ,R

#-----------更新環境--------------
def update_env(S, episode, step_counter):
  env_2Darray = np.zeros((x_len,y_len))#define env is a 2D SPACE, and define Location.
  env_2Darray[GOAL[0],GOAL[1]] = 6 
  if S == 'terminal':
    interaction = 'Episode %s: total_steps = %s' % (episode+1, step_counter) #回應
    print('\r{}'.format(interaction), end='') 
    time.sleep(20)                             
    print('\r                                ', end='') #清空
  else:
    env_2Darray[N_STATES_dict[S][0],N_STATES_dict[S][1]] = 5
    interaction = ''
    for i in range(x_len):
      interaction += ''.join(str(env_2Darray[i,:]))+'\n'
    print('\r{}'.format(interaction), end='')
    print('Episode %s: total_steps = %s' % (episode+1, step_counter)+'\n')
    time.sleep(FRESH_TIME)
    clear_output()

#----------建立reinforcement learning-----------
def rl():
    q_table = build_q_table(N_STATES, ACTIONS) #建立 Q table
    for episode in range(MAX_EPISODES): #從第一個回合玩到最後一個回合
        step_counter = 0
        S = 0 #初始情況，探索者放到左邊
        is_terminated = False 
        update_env(S, episode, step_counter) #更新環境
        while not is_terminated: #回合沒有結束

            A = choose_action(S, q_table) 
            S_, R = get_env_feedback(S, A)  
            q_predict = q_table.loc[S, A] #估計值
            if S_ != 'terminal': #回合還沒結束
                q_target = R + GAMMA * q_table.iloc[S_, :].max()   #真實值 
            else:
                q_target = R    
                is_terminated = True    # 結束這一回合
                
            q_table.loc[S, A] += ALPHA * (q_target - q_predict)  # update
            S = S_  # move to next state

            update_env(S, episode, step_counter+1)
            
            step_counter += 1
    return q_table

if __name__ == "__main__":
    q_table = rl()
    #print('\r\nQ-table:\n')
    print(q_table)











